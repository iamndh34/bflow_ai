import json
import os
import re

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import pandas as pd
import torch
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from openai import OpenAI


# D·ªØ li·ªáu ch·ª©c nƒÉng
features = [
    {
        "key": "sale_dashboard_revenue",
        "name_vi": "Doanh thu t·ªïng quan",
        "description": "Xem bi·ªÉu ƒë·ªì v√† th·ªëng k√™ doanh thu t·ªïng th·ªÉ theo th·ªùi gian.",
        "keywords": ["doanh thu", "revenue", "th·ªëng k√™", "bi·ªÉu ƒë·ªì", "dashboard"],
        "url": "https://docs.bflow.vn/sale/dashboard/revenue/"
    },
    {
        "key": "sale_dashboard_pipeline",
        "name_vi": "Pipeline b√°n h√†ng",
        "description": "Theo d√µi ti·∫øn tr√¨nh v√† tr·∫°ng th√°i c√°c c∆° h·ªôi b√°n h√†ng trong pipeline.",
        "keywords": ["pipeline", "c∆° h·ªôi", "giai ƒëo·∫°n", "b√°n h√†ng"],
        "url": "https://docs.bflow.vn/sale/dashboard/pipeline/"
    },
    {
        "key": "sale_revenue_plan",
        "name_vi": "K·∫ø ho·∫°ch doanh thu",
        "description": "L·∫≠p k·∫ø ho·∫°ch doanh thu, m·ª•c ti√™u b√°n h√†ng theo th√°ng/qu√Ω.",
        "keywords": ["doanh thu", "m·ª•c ti√™u", "doanh s·ªë", "d·ª± b√°o", "plan"],
        "url": "https://docs.bflow.vn/sale/revenue-plan/"
    },
    {
        "key": "sale_contact",
        "name_vi": "Li√™n h·ªá",
        "description": "Qu·∫£n l√Ω th√¥ng tin li√™n h·ªá v√† kh√°ch h√†ng ti·ªÅm nƒÉng.",
        "keywords": ["li√™n h·ªá", "kh√°ch h√†ng", "contact", "crm"],
        "url": "https://docs.bflow.vn/sale/contact/"
    },
    {
        "key": "sale_account",
        "name_vi": "T√†i kho·∫£n kh√°ch h√†ng",
        "description": "L∆∞u tr·ªØ th√¥ng tin kh√°ch h√†ng doanh nghi·ªáp, nh√≥m, ph√¢n lo·∫°i.",
        "keywords": ["account", "kh√°ch h√†ng", "doanh nghi·ªáp", "th√¥ng tin"],
        "url": "https://docs.bflow.vn/sale/account-tai-khoan/"
    },
    {
        "key": "sale_opportunity",
        "name_vi": "C∆° h·ªôi kinh doanh",
        "description": "Qu·∫£n l√Ω c∆° h·ªôi kinh doanh c·ªßa kh√°ch h√†ng, giai ƒëo·∫°n ch·ªët deal v√† pipeline.",
        "keywords": ["c∆° h·ªôi", "deal", "pipeline", "b√°n h√†ng", "kh√°ch h√†ng"],
        "url": "https://docs.bflow.vn/sale/opportunity/"
    },
    {
        "key": "sale_quotation",
        "name_vi": "B√°o gi√°",
        "description": "T·∫°o, ch·ªânh s·ª≠a v√† g·ª≠i b√°o gi√° cho kh√°ch h√†ng.",
        "keywords": ["b√°o gi√°", "quotation", "gi√°", "ƒë·ªÅ xu·∫•t"],
        "url": "https://docs.bflow.vn/sale/quotation/"
    },
    {
        "key": "sale_order",
        "name_vi": "ƒê∆°n h√†ng",
        "description": "Theo d√µi ƒë∆°n h√†ng t·ª´ b√°o gi√° t·ªõi giao h√†ng, thanh to√°n.",
        "keywords": ["ƒë∆°n h√†ng", "sale order", "b√°n h√†ng", "giao h√†ng"],
        "url": "https://docs.bflow.vn/sale/order/"
    },
    {
        "key": "sale_invoice",
        "name_vi": "H√≥a ƒë∆°n b√°n h√†ng",
        "description": "Xu·∫•t h√≥a ƒë∆°n, ghi nh·∫≠n c√¥ng n·ª£ v√† thanh to√°n t·ª´ kh√°ch h√†ng.",
        "keywords": ["h√≥a ƒë∆°n", "invoice", "thanh to√°n", "c√¥ng n·ª£"],
        "url": "https://docs.bflow.vn/sale/invoice/"
    },
    {
        "key": "sale_contract",
        "name_vi": "H·ª£p ƒë·ªìng",
        "description": "T·∫°o v√† qu·∫£n l√Ω h·ª£p ƒë·ªìng b√°n h√†ng v·ªõi kh√°ch h√†ng.",
        "keywords": ["h·ª£p ƒë·ªìng", "contract", "k√Ω k·∫øt", "b√°n h√†ng"],
        "url": "https://docs.bflow.vn/sale/contract/"
    },
    {
        "key": "sale_product",
        "name_vi": "S·∫£n ph·∫©m",
        "description": "Qu·∫£n l√Ω danh m·ª•c s·∫£n ph·∫©m, m√£ SKU v√† gi√° b√°n.",
        "keywords": ["s·∫£n ph·∫©m", "product", "h√†ng h√≥a", "gi√° b√°n"],
        "url": "https://docs.bflow.vn/sale/product/"
    }
]

# D·ªØ li·ªáu cho documents
files = [
    ("statics/tutorial-documents/user_login_tutorial.txt", "auth")
]

chunks = []
docs_texts = []

# ƒê·ªçc t·ª´ng file v√† t√°ch chunk
for file_path, doc_name in files:
    print(doc_name)
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    print(f"\nƒê·ªçc file '{file_path}' th√†nh c√¥ng ({len(text)} k√Ω t·ª±)")

    # T√°ch theo section l·ªõn (theo ti√™u ƒë·ªÅ # ...)
    sections = re.split(r'(?=\n# )', text)
    sections = [s.strip() for s in sections if s.strip()]

    # B·ªô chia nh·ªè chi ti·∫øt h∆°n
    sub_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        length_function=len,
        separators=["\n## ", "\n\n", "\n", ". ", " "]
    )

    for section in sections:
        if len(section) > 800:
            sub_chunks = sub_splitter.split_text(section)
            for sc in sub_chunks:
                chunks.append({"text": sc, "source": doc_name})
        else:
            chunks.append({"text": section, "source": doc_name})

print(f"\nT·ªïng s·ªë chunks sau khi t√°ch: {len(chunks)}")

# D·ªØ li·ªáu cho danh s√°ch s·∫£n ph·∫©m
products = [
    {"code": "M001", "name": "7 Up", "selected": False},
    {"code": "M002", "name": "Sting", "selected": False},
    {"code": "M003", "name": "Tiger B·∫°c Lon L·ªõn 330ml", "selected": False},
    {"code": "M004", "name": "KhƒÉn l·∫°nh", "selected": False},
    {"code": "M005", "name": "H√†u n∆∞·ªõng ph√¥ mai", "selected": False},
    {"code": "M006", "name": "Ngh√™u h·∫•p s·∫£", "selected": False},
    {"code": "M007", "name": "B√™ thui (b√≤ t∆°) (thƒÉn, qu·∫ø, b·∫Øp, g√π)", "selected": False},
    {"code": "M008", "name": "B√°nh tr√°ng n∆∞·ªõng", "selected": False},
    {"code": "M009", "name": "G·ªèi b√≤ t∆° b√≥p th·∫•u", "selected": False},
    {"code": "M010", "name": "B·∫°ch tu·ªôc n∆∞·ªõng sa t·∫ø", "selected": False},
    {"code": "M011", "name": "Rau th√™m B√™ thui", "selected": False},
    {"code": "M012", "name": "Mi·∫øn x√†o h·∫£i s·∫£n", "selected": False},
    {"code": "M013", "name": "C∆°m chi√™n h·∫£i s·∫£n", "selected": False},
    {"code": "M014", "name": "M·ª±c chi√™n n∆∞·ªõc m·∫Øm", "selected": False},
    {"code": "M015", "name": "T√¥m n∆∞·ªõng mu·ªëi ·ªõt", "selected": False},
    {"code": "M016", "name": "C√° l√≥c n∆∞·ªõng trui", "selected": False},
    {"code": "M017", "name": "L·∫©u th√°i h·∫£i s·∫£n", "selected": False},
    {"code": "M018", "name": "·ªêc h∆∞∆°ng n∆∞·ªõng m·ªçi", "selected": False},
    {"code": "M019", "name": "S√≤ ƒëi·ªáp n∆∞·ªõng m·ª° h√†nh", "selected": False},
    {"code": "M020", "name": "T√†u h·ªß chi√™n gi√≤n", "selected": False},
    {"code": "M021", "name": "Rau mu·ªëng x√†o t·ªèi", "selected": False},
    {"code": "M022", "name": "Canh chua c√° b√¥ng lau", "selected": False}
]

# D·ªØ li·ªáu cho accounting
path = r"statics/accounting_docs/account.xlsx"

data_json = []

if not os.path.exists(path):
    print("Kh√¥ng t√¨m th·∫•y file:", path)
else:
    df = pd.read_excel(path, header=0)

    df = df.dropna(how='all')
    df.columns = df.columns.str.strip()

    df = df.fillna("")
    data_json = df.to_dict(orient='records')

print(data_json)


# --- Kh·ªüi t·∫°o model v√† FAISS index ch·ªâ m·ªôt l·∫ßn ---
print("ƒêang load model SentenceTransformer...")
_model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')
print("Model loaded th√†nh c√¥ng!")


# Embedding feature
features_texts = [f"{f['name_vi']} {f['description']} {' '.join(f['keywords'])}" for f in features]
features_embeddings = _model.encode(features_texts)
features_dimension = features_embeddings.shape[1]
features_index = faiss.IndexFlatL2(features_dimension)
features_index.add(np.array(features_embeddings).astype('float32'))
print("FAISS index ƒë√£ s·∫µn s√†ng cho features!")


# Embedding documents
docs_texts = [c["text"] for c in chunks]
docs_embeddings = _model.encode(docs_texts, convert_to_numpy=True, show_progress_bar=True)
docs_dimension = docs_embeddings.shape[1]
docs_index = faiss.IndexFlatL2(docs_dimension)
docs_index.add(np.array(docs_embeddings).astype("float32"))
print("FAISS index ƒë√£ s·∫µn s√†ng cho documents!")


# Embedding productions
products_texts = [f"{f['code']} {f['name']}" for f in products]
products_embeddings = _model.encode(products_texts)
products_dimension = products_embeddings.shape[1]
products_index = faiss.IndexFlatL2(products_dimension)
products_index.add(np.array(products_embeddings).astype('float32'))
print("FAISS index ƒë√£ s·∫µn s√†ng cho products!")

# Embedding accouting
accounting_texts = [
    f"T√†i kho·∫£n {d['AccountNumber']}, T√™n ti·∫øng vi·ªát {d['VietnameseName']}, t√™n ti·∫øng anh {d['EnglishName']}, √ù nghƒ©a {d['Meaning']}, S·ª≠ d·ª•ng {d['Usage']}"
    for d in data_json]
accounting_embeddings = _model.encode(accounting_texts)
accounting_dimension = accounting_embeddings.shape[1]
accounting_index = faiss.IndexFlatL2(accounting_dimension)
accounting_index.add(np.array(accounting_embeddings).astype('float32'))
print("FAISS index ƒë√£ s·∫µn s√†ng cho accounting!")


# --- H√†m ch√≠nh ---
class RAGFeature:
    @classmethod
    def rag_feature(cls, user_input: str, top_k: int = 1):
        try:
            user_embedding = _model.encode([user_input])
            D, I = features_index.search(np.array(user_embedding).astype('float32'), k=top_k)
            print(user_input)
            results = []
            for idx, dist in zip(I[0], D[0]):
                func = features[idx]
                results.append({
                    "key": func["key"],
                    "name_vi": func["name_vi"],
                    "description": func["description"],
                    "url": func["url"],
                    "distance": float(dist)
                })

            for result in results:
                print(result)
            return results[0] if results else None

        except Exception as e:
            print("L·ªói RAG:", e)
            return None

class RAGDocument:
    @classmethod
    def find_best(cls, user_input, top_k=3, doc=None):
        query_vec = _model.encode([user_input])
        distances, indices = docs_index.search(np.array(query_vec).astype("float32"), len(chunks))
        results = []
        for i, idx in enumerate(indices[0]):
            result = {
                "distance": float(distances[0][i]),
                "text": chunks[idx]["text"],
                "source": chunks[idx]["source"]
            }
            print(result)
            results.append(result)
        if doc:
            results = [r for r in results if r["source"] == doc]

        results = sorted(results, key=lambda x: x["distance"])[:top_k]
        return [r["text"] for r in results]

    @classmethod
    def synthesize_answer(user_query, retrieved_texts, api_key):
        """
        T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi b·∫±ng GPT-4o-mini d·ª±a tr√™n th√¥ng tin t√¨m th·∫•y.
        """
        client = OpenAI(api_key=api_key)
        context = "\n\n".join(retrieved_texts)

        prompt = f"""
    Ng∆∞·ªùi d√πng h·ªèi: {user_query}

    D∆∞·ªõi ƒë√¢y l√† c√°c ƒëo·∫°n n·ªôi dung t√†i li·ªáu c√≥ li√™n quan:

    {context}

    H√£y t·ªïng h·ª£p m·ªôt c√¢u tr·∫£ l·ªùi sinh ƒë·ªông, ch√≠nh x√°c, t·ª± nhi√™n b·∫±ng ti·∫øng Vi·ªát d∆∞·ªõi d·∫°ng html th·∫≠t ƒë·∫πp kh√¥ng c·∫ßn th·∫ª html ƒë·∫ßu, kh√¥ng c·∫ßn ch·ªânh color.
    N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y n√≥i r√µ "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ch√≠nh x√°c trong t√†i li·ªáu."
    """

        print("\n ƒêang t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi b·∫±ng GPT-4o-mini...\n")
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()

class RAGProduct:
    @classmethod
    def rag_products(cls, product_list_input, top_k=1, threshold=0.6):
        # Reset flag
        for p in products:
            p["selected"] = False
            p["unit"] = ""
            p["quantity"] = 0
            p["unit_price"] = 0
            p["vat_amount"] = 0
            p["vat_rate"] = "0"
            p["similarity"] = 0.0

        # D√≤ t√¨m match
        for item in product_list_input:
            name = item.get("name", "").strip()
            if not name:
                continue

            user_emb = _model.encode([name], normalize_embeddings=True)
            D, I = products_index.search(np.array(user_emb).astype("float32"), k=top_k)

            best_idx = int(I[0][0])
            best_sim = float(D[0][0])
            best_product = products[best_idx]

            print(f"\nüîπ OCR: {name}")
            print(f"- Match: {best_product['name']} (similarity={best_sim:.4f})")

            if best_sim >= threshold:
                best_product["selected"] = True
                best_product["unit"] = item.get("unit", "")
                best_product["quantity"] = item.get("quantity", 0)
                best_product["unit_price"] = item.get("unit_price", 0)
                best_product["vat_amount"] = item.get("vat_amount", 0)
                best_product["vat_rate"] = item.get("vat_rate", "0")
                best_product["similarity"] = round(best_sim, 4)

        json_output = json.dumps(products, ensure_ascii=False, indent=4)
        print("\n Danh s√°ch product sau khi match:")
        print(json_output)

        return json_output

class RAGAccounting:
    @classmethod
    def rag_accounting(cls, user_input, top_k=1, threshold=50):
        try:
            user_embedding = _model.encode([user_input])
            D, I = accounting_index.search(np.array(user_embedding).astype('float32'), k=top_k)
            print(user_input)
            results = []
            for idx, dist in zip(I[0], D[0]):
                if dist < threshold:
                    da = data_json[idx]
                    results.append({
                        "AccountNumber": da["AccountNumber"],
                        "VietnameseName": da["VietnameseName"],
                        "EnglishName": da["EnglishName"],
                        "Meaning": da["Meaning"],
                        "AccountingMethod": da["AccountingMethod"],
                        "Usage": da["Usage"],
                        "CorrespondingAccounts": da["CorrespondingAccounts"],

                        "distance": float(dist)
                    })

            for result in results:
                print(result)
            return results[0] if results else None

        except Exception as e:
            print("L·ªói RAG:", e)
            return None

